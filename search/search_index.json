{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> to the next service window</p> <p>We will be performing a scheduled maintenance on the AI Student Cloud. The routine maintenance will take place  between 00:01 and 23:59. AI Student Cloud will be unavailable throughout most of the day. You can still submit new jobs until the beginning of the service window. For jobs that may exceed the service window, please ensure to set a maximum runtime using the parameter <code>--time</code> that concludes before 23:59 the day preceding the service window. Read more about the <code>--time</code> parameter here. Otherwise, these jobs will not be able to start until after the maintenance period. You will receive email notifications 1 month, 14 days, and 1 day prior to the scheduled maintenance window.</p> <p>If you have any further questions please refer your question to CLAAUDIA through the AAU service portal.</p>"},{"location":"#welcome-to-ai-student-cloud-documentation","title":"Welcome to AI Student Cloud documentation","text":"<p>Welcome to the AI Student Cloud documentation, a guide designed to help Aalborg University students delve into powerful computing projects using AI Student Cloud. Before getting started we recommend getting an overview of the system behind AI Student Cloud and reading our Guidelines.</p> <p></p> <ul> <li> <p> System overview</p> <p>The Architecture of AI Student Cloud</p> <p> System overview</p> </li> <li> <p> Guidelines</p> <p>Best practices for using AI Student Cloud</p> <p> Guidelines</p> </li> <li> <p> Getting Started</p> <p>Step-by-step guide to AI Student Cloud</p> <p> Getting Started</p> </li> <li> <p> Courses</p> <p>AI Student Cloud applications courses</p> <p> Courses</p> </li> </ul> <p></p>"},{"location":"#what-is-ai-student-cloud","title":"What is AI Student Cloud?","text":"<p>The AI Student Cloud is designed exclusively for students at Aalborg University, offering high-performance computing (HPC) right at your fingertips. Think of it as a mini supercomputer, packed with GPUs, making it a perfect playground for training deep learning models, running simulations, and performing high-speed data analysis.</p> <p>This platform is home to an extensive collection of GPU resources, tailored specifically for machine learning tasks. Whether you're working on image recognition, deep learning tasks, or data processing, AI Student Cloud is equipped to handle vast processes that benefit from parallel computing.</p> <p> Purpose</p> <p>AI Student Cloud isn't just about providing hardware; it's about opening the door to high-performance computing for students, with an educational twist. From simulations to deep learning, it supports a large number of applications, all while fostering an environment where students can experiment, learn, and grow their computational skills. We advocate reading the Guidelines of AI Student Cloud before getting started.</p> <p> How to access</p> <p>First you need to fill out an application formCHANGE LINK to request for access. After getting approval, you can access AI Student Cloud using a terminal application from your computer to log into the front-end node. This is where you'll manage files and submit jobs to be processed on the GPU nodes. It's a shared resource, emphasizing learning how to navigate a queueing mechanism and understand containerization. We will guide you through all this in Getting Started.</p> <p> Who manage AI Student Cloud</p> <p>AI Student Cloud is managed by the CLAAUDIA team. CLAAUDIA is a specialized team within ITS at Aalborg University focused on research data support, particularly skilled in leveraging high-performance computing and cloud resources such as AI Student Cloud. They offer support and consultations to help students and researchers navigate through options for utilizing supercomputing resources effectively.</p>"},{"location":"faq/","title":"FAQ","text":"<p>How long can I use AI Student Cloud for a project</p> <p>...</p> <p>Can I have a graphical user interface (GUI) in AI Student Cloud?</p> <p>No. As a rule of thumb, you can only operate applications through a command line interface. AI Student Cloud is best suited for throughput type computing: queue up your job and come back for the result later. If your project could benefit from running on a GPU cluster but demands an interactive graphical interface, contact CLAAUDIA research support at support@its.aau.dk.</p> <p>What software can I run in AI Student Cloud?</p> <p>In principle any software that can run under Linux since the operating system on the servers in AI Student Cloud is Linux. In many cases, it is necessary to encapsulate the software in a container in order to use it. The software must be able to utilise the (NVIDIA) GPUs in the platform. The GPUs cannot automatically accelerate software that was not designed for it.</p> <p>Can I use AI Student Cloud as a collaborative platform?</p> <p>Yes - it is possible to share files among different users on the cluster.</p> <p>Am I allowed to work with confidential or sensitive data on AI Student Cloud?</p> <p>INDS\u00c6T LINK TIL DATA KLASSIFIKATION No. In AI Student Cloud you are only allowed to work with public or internal information according to AAU\u2019s data classification model (classified as levels 0 and 1, respectively).</p> <p>A similar solution for working with confidentoinal or sensitive data (classified as levels 2 and 3) is being developed.</p> <p>Who can have access to the AI Student Cloud platform?</p> <p>UPDATE All researchers at AAU as well as students in relation to relevant projects when approved by supervisor.</p> <p>Why do I get <code>\\r</code> error when running my .py script?</p> <p>If you encounter an error message like: <code>/usr/bin/env python3/r: bad interpreter: No such file or directory</code> while running a .py file, it might be because the file was edited on your local Windows computer before moving it to AI Student Cloud. Line endings often get converted when files are moved between Linux and Windows. This conversion is a frequent issue as Linux and Unix-like systems use <code>\\n</code> for line breaks, whereas Windows uses <code>\\r\\n</code> (CRLF, Carriage Return + Line Feed). </p> <p>Solution: In code editors such as VS Code or PyCharm, you can switch between LF (Linux endings) and CRLF (Windows endings) from the right-hand side of the status bar at the bottom of the window. Therefore, use LF endings if you wish to move a file to AI Student Cloud.</p>"},{"location":"glossery/","title":"Glossery","text":""},{"location":"glossery/#hpc","title":"HPC","text":"<p>High-performance computing (HPC) uses powerful resources to perform complex, data-intensive tasks beyond a single computer's capacity. HPC systems include multiple processors, large memory, and specialized networking for rapid data exchange. Researchers can simulate, model, and analyse data at unprecedented speed and scale.</p>"},{"location":"glossery/#deep-learning","title":"Deep learning","text":"<p>The AI Cloud is a facility consisting of several servers designed to be ideal for training deep learning algorithms. Deep learning is in many cases branded as artificial intelligence (AI) - hence the name AI Cloud. This also makes the facility good for a wide range of computationally intensive work such as numerical simulations and high-performance data analysis (HPDA). See \"Overview\" in the menu above for more details on what the AI Cloud consists of.</p>"},{"location":"glossery/#parallel-computing","title":"Parallel Computing","text":"<p>Parallel computing refers to the simultaneous execution of multiple tasks or parts of a single task across multiple processing units, such as CPUs or GPUs. This approach significantly accelerates computational tasks by dividing the workload among several processors, thereby reducing overall processing time.</p>"},{"location":"glossery/#front-end-node","title":"Front-end node","text":"<p>The front-end node is used for logging into the platform, accessing your files, and starting jobs on the compute nodes. The front-end node is a relatively small server which is not meant for performing heavy computations; only light-weight operations such as transferring files to and from AI Student Cloud and defining and launching job scripts.</p>"},{"location":"glossery/#computing-cluster","title":"Computing cluster","text":"<p>A computing cluster is a collection of interconnected computers (compute nodes) that work together as a single, integrated computing resource. This setup is designed to handle large computational tasks more efficiently than a single computer could. Clusters are used for tasks that require high performance and availability, such as scientific simulations, data analysis, and large-scale web services.</p>"},{"location":"glossery/#compute-nodes","title":"Compute nodes","text":"<p>Compute Nodes are the individual computers within the Computing cluster that perform the actual computational work. Each compute node is a separate computer, equipped with its own processors (CPUs), memory (RAM), and GPUs for specialized computations. Compute nodes are networked together and run a unified operating system and software environment. Compute nodes are tasked with executing the computational jobs submitted by users. In AI Student Cloud, users don't interact directly with compute nodes. Instead, they submit jobs through the job scheduler Slurm, which then allocates the necessary resources and runs the job on the compute nodes.</p>"},{"location":"glossery/#slurm","title":"Slurm","text":"<p>The Slurm queue system is AI Student Clouds job scheduling and management tool commonly used in HPC environments. It efficiently allocates computing resources by prioritizing and scheduling jobs submitted by users based on their requirements and available resources. Through Slurm, users can submit their computational tasks to a centralized queue, allowing for fair resource distribution and optimal utilization of the HPC system.</p>"},{"location":"glossery/#containers","title":"Containers","text":"<p>Containers provide a streamlined and reproducible environment for running applications by encapsulating all necessary dependencies, libraries, and configurations. This ensures consistent performance across different computing nodes within AI Student Cloud, regardless of underlying system configurations. By adopting containerization, users can easily deploy and manage complex software stacks, enabling efficient utilization of computational resources and facilitating collaboration among students.</p>"},{"location":"guidelines/","title":"Guidelines","text":"<p>AI Student Cloud is a service made available to all AAU students. The platform consists of shared hardware, and this only works if the users use the platform as intended. This is why we ask you to consider the following use guidelines.</p> <p>Data deletion and extension policies</p> <p>At the semester's end, all your data and user information will be automatically removed from the AI Student Cloud platform. For the spring semester, the deletion date is June 31st, and for the autumn semester, December 31st. You will receive email notifications 1 month, 14 days, and 1 day prior to the deletion date. It is important to ensure that you transfer any desired files from the AI Student Cloud to your local computer before the deletion date.</p> <p>Should you wish to keep your data on AI Student Cloud and use the platform for another semester, you can submit an extension request through the application formUPDATE for the AI Student Cloud. Alternatively, you can apply for a new project when the next semester begins.</p> <p>Not for confidential or sensitive data</p> <p>In AI Student Cloud you are only allowed to work with public or internal information according to AAU\u2019s data classification model (classified as levels 0 and 1, respectively).</p> <p>If you would like to work with confidential or sensitive data (classified as levels 2 and 3), then we support another HPC platform called UCloud.</p> <p>Data Management best practices</p> <p>Effective data management is crucial for maximizing the value of your research and ensuring its integrity. Organize your data in a clear and structured manner, labeling files and directories appropriately. Regularly back up your data to prevent loss in case of unexpected events. Additionally, adhere to any data sharing and privacy regulations applicable to your field. Get more guidance about data management here.</p> <p>Not intended for CPU processing</p> <p>The platform excels in tasks requiring parallel processing capabilities, such as training deep learning models, image and video analysis, and other GPU-accelerated computational work. However, it's not intended for applications that only need CPU processing.</p> <p>Allocating the right amount of resources</p> <p>Please be mindful of your allocations and refrain from allocating many resources without knowing/testing/verifying that you indeed can utilise all of the allocated resources. </p> <p>Please be mindful and de-allocate the resources when you are no longer using them. This enables other users to run their jobs.</p> <p>Focus on data processing, not storage</p> <p>AI Student Cloud is designed primarily for data processing, not storage. Our High-Performance Computing (HPC) platform supports complex data analysis and simulation tasks efficiently. This focus ensures our resources are distributed optimally among users.</p> <p>Scheduled maintenance</p> <p>On specific dates, scheduled maintenance will be conducted on the AI Student Cloud. The routine maintenance will occur between 00:01 and 23:59. AI Student Cloud will be unavailable throughout most of that day. You can still submit new jobs until the beginning of the service window. For jobs that may exceed the service window, please ensure to set a maximum runtime using the parameter <code>--time</code> that concludes before 23:59 the day preceding the service window. Read more about the <code>--time</code> parameter here. Otherwise, these jobs will not be able to start until after the maintenance period. You will receive email notifications 1 month, 14 days, and 1 day prior to the scheduled maintenance window.</p>"},{"location":"support/","title":"Support","text":"<p>The platform offers support through the CLAAUDIA team. We can help you diagnose and resolve general issues you might encounter while using AI Student Cloud. For all questions and requests related to the availability, use and troubleshooting of AI Student Cloud, please contact us through the official AAU service portal. Stj\u00e6l fra Sven - noget med, refer to CLAAUDIA...</p> <p>A good practice for asking a question</p> <p>To ensure optimal technical assistance, we advise you follow these recommended practices when submitting questions and requests:</p> <ol> <li>Use a detailed subject line. Doing so facilitates the quick identification of the problem and connects it with similar incidents previously reported by other users.</li> <li>Clarify what you have done so far and which operating system you use. Include specifics like which applications you are using. This detail aids in accurately recreating your environment to better understand the issue.</li> <li>Explain the issue and what you intended to achieve. Provide a comprehensive description of the issue, noting what has worked up to that point and the steps you've taken in an attempt to resolve it.</li> </ol>"},{"location":"system-overview/","title":"System overview","text":"<p> Front-end node</p> <p>You start by logging into a front-end node. This is the gateway to the HPC system. Here, you can manage files, write and edit code, and prepare your computational tasks. The front-end node is not for heavy computations but for preparation and interaction with the HPC system.</p> <p></p> <p> File storage</p> <p>The AI Student Cloud stores your files in your user directory. Your user directory is stored on a network file system that allows all of the nodes within the platform can access your files. This means that if you store or edit a file in your user directory on the front-end node, the compute nodes can see the same file and contents thereof.</p> <p></p> <p> Queue system</p> <p>After preparing your computational task, you submit it to AI Student Clouds queue system, Slurm. Slurm is like a traffic controller. It receives job submissions from users and decides when and where to run each job based on available resources and the job's requirements. Your job is placed in a queue until resources are available.</p> <p></p> <p> Containers</p> <p>One job could be to deploy an applications such as TensorFlow. All applications on AI Student Cloud must be obtained in containers. Containers are like virtual packages that hold all the stuff your program needs to run smoothly. They bundle up everything \u2013 the code, libraries, and settings.</p> <p></p> <p> Compute nodes</p> <p>Once resources are available, Slurm dispatch the submitted job to specific compute nodes within the AI Student Cloud computing cluster.</p> <p><pre><code>flowchart LR\n  subgraph id1[&lt;p style=\"font-family: Barlow, sans-serif; font-weight: 800; font-size: 12px; text-transform: uppercase; color: #221a52; letter-spacing: 1px; margin: 5px;\"&gt;Computing cluster&lt;/p&gt;]\n    direction TB\n    A[\"&lt;span&gt;&lt;img src=\"/assets/img/server.svg\"  width='25' height='25' &gt;Compute nodes A256-t4-[01-03]&lt;/span&gt;\"] ~~~ B[\"&lt;span&gt;&lt;img src=\"/assets/img/server.svg\" width='25' height='25'&gt;Compute nodes A256-t4-[01-03]&lt;/span&gt;\"] ~~~ C[\"&lt;span&gt;&lt;img src=\"/assets/img/server.svg\" width='25' height='25'&gt;Compute nodes A256-t4-[01-03]&lt;/span&gt;\"] ~~~ D[\"&lt;span&gt;&lt;img src=\"/assets/img/server.svg\" width='25' height='25'&gt;Compute nodes A256-t4-[01-03]&lt;/span&gt;\"]\n    end\n\n  subgraph id2[&lt;p style=\"font-family: Barlow, sans-serif; font-weight: 800; font-size: 16px; text-transform: uppercase; color: #221a52; letter-spacing: 1px; margin: 10px;\"&gt;AI Student Cloud&lt;/p&gt;]\n    direction LR\n    G[\"&lt;span&gt;&lt;img src=\"/assets/img/server.svg\" width='25' height='25'&gt;Front-end node (a256-t4-01)&lt;/span&gt;\"] --&gt; id1 &lt;--&gt; E[&lt;span&gt;&lt;img src=\"/assets/img/database.svg\" width='25' height='25'&gt;File storage&lt;/span&gt;]\n    end\n\n  F[&lt;span&gt;&lt;img src=\"/assets/img/person.svg\" width='25' height='25'&gt;User&lt;/span&gt;]-- SSH --&gt; G</code></pre> UPDATE TO MATCH AI STUDENT CLOUD SYSTEM</p>"},{"location":"system-overview/#overview-of-compute-nodes","title":"Overview of compute nodes","text":"<p>AI Student Cloud is a heterogeneous platform with several different types of hardware available in the compute nodes and currently include:</p> Node name CPU System RAM GPU a256-t4-01 2x AMD EPYC 7302 16-core 256GB 6x NVIDIA T4 GPUs, 16GB RAM .............. ............................ .......... ............................... <p>This diverse selection of different hardware in the AI Student Cloud allows for more suitable choice of specific hardware according to your task. MAYBE EXPLAIN EXAMPLES OF WHAT JOBS DIFFERENT NODES WOULD BE FIT FOR</p>"},{"location":"system-overview/#list-of-pre-installed-software","title":"List of pre-installed software","text":"<pre><code>SingularityCE 4.1.2\nSlurm 23.11\nPython 3.9.2\nMicro\n</code></pre> <p>UPDATE LIST</p>"},{"location":"additional-guides/checking-the-queue/","title":"Checking the queue","text":"<p>When using the cluster, you typically wish to see an overview of what is currently in the queue. For example to see how many jobs might be waiting ahead of you or to get an overview of your own jobs.</p> <p>The command <code>squeue</code> can be used to get a general overview:</p> <p>Example</p> <p>NEED TO BE UPDATED TO AI STUDENT CLOUD SPECIFICS</p> <pre><code>squeue\n\nJOBID(1) PARTITION(2)       NAME(3)  USER(4) ST(5) TIME(6)  NODES(7) NODELIST(REASON)(8)\n31623        batch          DRSC xxxxxxxx     R 6:45:14         1         i256-a10-10\n31693        batch      singular yyyyyyyy     R   24:20         1         i256-a40-01\n31694        batch      singular yyyyyyyy     R   24:20         1         i256-a40-01\n31695        batch      singular yyyyyyyy     R   24:20         1         i256-a40-01\n31696        batch      singular yyyyyyyy     R   24:20         1         i256-a40-01\n31502    prioritiz       runQHGK.zzzzzzzz    PD    0:00         1        (Dependency)\n31504    prioritiz       runQHGK.zzzzzzzz    PD    0:00         1        (Dependency)\n</code></pre> <ol> <li><code>JOBID</code> shows the ID number of each job in queue.</li> <li><code>PARTITION</code> shows which partition each job is running in.</li> <li><code>NAME</code> is the name of the job which can be specified by the user creating it.</li> <li><code>USER</code> is the username of the user who created the job.</li> <li><code>ST</code> is the current state of each job; for example \"R\" means a job is running and \"PD\" means pending. There are other states as well - see <code>man squeue</code> for more details (under \"JOB STATE CODES\").</li> <li><code>TIME</code> shows how long each job has been running. NODES shows how many nodes are involved in each job allocation.</li> <li><code>NODES</code> shows how many nodes are involved in each job allocation.</li> <li><code>NODELIST</code> shows which node(s) each job is running on, or alternatively, why it is not running yet.</li> </ol> <p>Showing your own jobs only:</p> <p>Example</p> <pre><code>squeue --me\n</code></pre> <p><code>squeue</code> can show many other details about jobs as well. Run <code>man squeue</code> to see detailed documentation on how to do this.</p>"},{"location":"additional-guides/checking-the-status-of-compute-nodes/","title":"Checking the status of compute nodes","text":"<p>It is often desirable to monitor the resource status of the compute nodes when you wish to run a job. </p> <p>Example</p> <p>NEED TO BE UPDATED TO AI STUDENT CLOUD SPECIFICS The <code>sinfo</code> command shows basic information about partitions in the queue system and what the states of nodes in these partitions are.</p> <pre><code>sinfo\n\nPARTITION (1)   AVAIL (2)  TIMELIMIT (3)  NODES (4)  STATE (5) NODELIST (6)\nbatch*             up       12:00:00          1        mix     nv-ai-04\nbatch*             up       12:00:00          8       idle     a256-t4-[01-02],i256-a10-06,i256-a40-[01-02]...\nprioritized        up     6-00:00:00          8       idle     a256-t4-[01-02],i256-a10-06,i256-a40-[01-02]...\n</code></pre> <ol> <li><code>PARTITION</code> In the context of the AI Student Cloud, partitions can be understood as distinct categories or groups of compute nodes, essentially serving as separate queues for jobs. Each partition defines a set of conditions under which jobs can access these nodes. It's possible for the same compute node to be a part of multiple partitions, like <code>batch</code> and <code>prioritized</code>. These will be explained later.</li> <li><code>AVAIL</code> shows the availability of the partition where \"up\" is normal, working state where you can submit jobs to it.</li> <li><code>TIMELIMIT</code> shows the time limit imposed by each partition in <code>HH:MM:SS</code> format.</li> <li><code>NODES</code> shows how many nodes are in the shown state in the specific partition.</li> <li><code>STATE</code> shows which state the listed nodes are in: <code>mix</code> means that the nodes are partially full - some jobs are running on them and they still have available resources; <code>idle</code> means that they are completely vacant and have all resources available; <code>allocated</code> means that they are completely occupied. Many other states are possible, most of which mean that something is wrong.</li> <li><code>NODELIST</code> shows the specific compute nodes that is affected by the job.</li> </ol> <p>You can also use the command <code>scontrol show node</code> or <code>scontrol show node &lt;node name&gt;</code> to show details about all nodes or a specific node, respectively.</p> <p>Example</p> <p>NEED TO BE UPDATED TO AI STUDENT CLOUD SPECIFICS <pre><code>scontrol show node a256-t4-01\n\nNodeName=a256-t4-01 Arch=x86_64 CoresPerSocket=16\nCPUAlloc=12 CPUTot=64 CPULoad=0.50\nAvailableFeatures=(null)\nActiveFeatures=(null)\nGres=gpu:t4:6\nNodeAddr=172.21.212.130 NodeHostName=a256-t4-01.srv.aau.dk Version=21.08.8-2\nOS=Linux 5.4.0-170-generic #188-Ubuntu SMP Wed Jan 10 09:51:01 UTC 2024\n...\n</code></pre></p> <p>The two commands <code>sinfo</code> and <code>scontrol show node</code> provide information which is either too little or way too much detail in most situations. As an alternative, we provide the tool <code>nodesummary</code> to show a hopefully more intuitive overview of the used/available resources in AI Student Cloud.</p> <p>SCREENSHOT NEED TO BE UPDATED TO AI STUDENT CLOUD SPECIFICS </p>"},{"location":"additional-guides/creating-a-conda-environment/","title":"Creating a conda environment","text":"<p>Creating a conda environment in a container may be easily done using cotainr. </p> <p>About cotainr</p> <p>cotainr is a tool developed by DeiC to ease building of Singularity containers. It can be used to build custom containers with additional software installable by Conda and Pip. This means it is primarily for adding Python packages to a container. It works from a base container image that you specify and then build additional Anaconda and pip packages which you supply as a conda environment specification.</p> <p>To begin utilizing cotainr, you'll first need to download the latest release from the Cotainr repository (as of late 2023.11.0) using <code>wget</code>:</p> <pre><code>wget https://github.com/DeiC-HPC/cotainr/archive/refs/tags/2023.11.0.zip\n</code></pre> <p>You should now have <code>2023.11.0.zip</code> in your user directory on AI Student Cloud. Unzip it:</p> <pre><code>unzip 2023.11.0.zip\n</code></pre> <p>You can now get access to the cotainr command line interface by using the path <code>cotainr-2023.11.0/bin/cotainr</code>. But first we will create a conda environment file, <code>conda_env.yml</code> that contains the conda channels/repositories and packages you need:</p> <p>Type <code>nano</code> and press <code>ENTER</code> (or use the editor of your choice), and enter the packages of your choice in the editor. In this example we will install <code>python=3.11.0</code> and <code>numpy=1.23.5</code>:</p> <pre><code>channels:\n  - conda-forge\ndependencies:\n  - python=3.11.0\n  - numpy=1.23.5\n</code></pre> <p>Instaling pip packages</p> <p>Cotainr does not allow the direct creation of a container from a pip requirements.txt file. Nevertheless, pip packages can be integrated into a conda environment. For instance, by updating <code>conda_env.yml</code> to include them.</p> <pre><code>channels:\n  - conda-forge\ndependencies:\n  - python=3.11.0\n  - numpy=1.23.5\n  - pip\n  - pip:\n    - scipy==1.9.3\n</code></pre> <p>Save by pressing <code>CTRL + O</code> enter a file name, e.g. <code>conda_env.yml</code> and exit by pressing <code>CTRL + X</code>. Now you should have <code>conda_env.yml</code> in your directory. </p> <p>We can now build a container (Lets call it <code>conda_container.sif</code>) containing the conda environment specified in <code>conda_env.yml</code> with the following command:</p> <pre><code>srun cotainr-2023.11.0/bin/cotainr build conda_container.sif --base-image=docker://ubuntu:22.04 --conda-env=conda_env.yml --accept-licenses\n</code></pre> <p>Info</p> <p><code>--base-image=docker://ubuntu:22.04</code> is used because we have to use a base image in which bash is installed, like Ubuntu 22.04 image. </p> <p><code>--accept-licenses</code> is used to acknowledge the Miniforge license terms.</p> <p>After some time you should have <code>conda_container.sif</code> image in your directory. </p> <p>You can access the conda image and run code using the dependencies you set up. Lets try to see if it works by printing the numpy version:</p> <pre><code>srun singularity exec conda_container.sif python3 -c \"import numpy; print(numpy.__version__)\"\n</code></pre> <p>The terminal should now print <code>1.23.5</code>.</p>"},{"location":"additional-guides/download-containers-from-ngc/","title":"Download containers from NGC","text":"<p>You can download a large range of containers by visiting NVIDIA GPU Cloud (NGC) and check whether NVIDIA provides a container with the application you need.</p> <p></p> <p>As an example, this could be TensorFlow. You can search on NGC and find TensorFlow. Here you can choose the desired version from the \"Copy image path\" dropdown menu:</p> <p></p> <p>This copies a link to the container which we will use in the following example.</p> <p>Example</p> <p>We need to use Singularity to download the container and in order to run Singularity, we must run it through the Slurm queueing system using the command <code>srun</code>. </p> <p>To download the container to your AI Student Cloud instance paste the url to the image like so:</p> <p><code>srun --mem 40G singularity pull docker://nvcr.io/nvidia/tensorflow:24.03-tf2-py3</code></p> <p>NOTE: The container could take ~20 minutes to download. </p> <p>The above example consists of the following parts:</p> <ul> <li><code>srun</code>: the Slurm command which gets the following command executed on a compute node.</li> <li><code>mem</code>: a Slurm command that allows you allocate memory to your process, in this case 40GB of memory. A higher amount of memory than the default is needed specifically for this TensorFlow container. Please see Download memory consuming containers page for a better way to avoid excessive memory requirements.</li> <li><code>singularity pull</code>: the Singularity command which downloads a specified container.</li> <li><code>docker://nvcr.io/nvidia/tensorflow:24.03-tf2-py3</code>: this part of the command itself consists of two parts. <code>docker://</code> tells Singularity that we are downloading a Docker container and Singularity automatically converts this to a Singularity container upon download. <code>nvcr.io/nvidia/tensorflow:24.03-tf2-py3</code> is container label copied from the NGC webpage which identifies the particular container and version that we want.</li> </ul> <p>Once the <code>singularity pull</code> command has completed, you should have a file called <code>tensorflow_24.03-tf2-py3.sif</code> in your user directory (use the command <code>ls</code> to see the files in your current directory).</p> <p>Work-around for memory-consuming downloads</p> <p>There is a workaround if you have problems downloading containers due to excessive memory requirements. See out guide: Download memory consuming containers.</p>"},{"location":"additional-guides/download-memory-consuming-containers/","title":"Download memory consuming containers","text":"<p>Downloading some Singularity containers in AI Student Cloud may require a large amount of memory to succeed, such as:</p> <pre><code>srun singularity pull docker://nvcr.io/nvidia/tensorflow:23.03-tf1-py3\n</code></pre> <p>If you simply run this command as-is, you are likely to experience an out-of-memory error during build of the container image. A work-around for this issue is to simply allocate more memory to your job using the <code>--mem</code> option for <code>srun</code> (it could easily require 40-50GB). This may cause your job to have to wait for a long time before it can start. </p> <p>There is, however, a better way to run Singularity to avoid the unreasonable memory requirement. Please follow these steps to use the <code>/tmp</code> partition of a compute node during build of your container:</p> <ol> <li> <p>Start an interactive job for building your container: <pre><code>srun --pty bash -l\n</code></pre>    (You may add the <code>--nodelist</code> parameter to request a particular compute node as usual with <code>srun</code>.)</p> </li> <li> <p>Create a temporary directory for yourself to use during build of your container image: <pre><code>mkdir /tmp/`whoami`\n</code></pre> Take note of the back-tick characters in the above command; this is just to create a directory called <code>/tmp/username</code> if your username is <code>username</code>. You can call it something else instead, but it is important to create it under <code>/tmp</code>.</p> </li> <li> <p>Run Singularity to build your container image, using your new directory in <code>/tmp</code> for temporary data storage: <pre><code>SINGULARITY_TMPDIR=/tmp/`whoami` singularity pull docker://nvcr.io/nvidia/tensorflow:23.03-tf1-py3\n</code></pre> The <code>SINGULARITY_TMPDIR</code> variable should be set to whatever you named your temporary directory in step 2.</p> </li> <li> <p>After Singularity has finished building, delete your temporary directory: <pre><code>rm -r /tmp/`whoami`\n</code></pre></p> </li> <li> <p>Exit your interactive job: <pre><code>exit\n</code></pre></p> </li> </ol>"},{"location":"additional-guides/running-a-container-in-interactive-mode/","title":"Running a container in interactive mode","text":"<p>You can launch a shell within a Singularity container, allowing you to interact with the container's environment by using the <code>shell</code> command:</p> <p>Example</p> <pre><code>srun --gres=gpu:1 --pty singularity shell --nv tensorflow_24.03-tf2-py3.sif\n</code></pre> <p>The <code>--pty</code> creates a virtual interactive terminal for a command to run within.</p> <p>You now have shell access</p> <pre><code>Singularity&gt;\n</code></pre> <p>Lets try checking the Python version:</p> <pre><code>python3 --version\n</code></pre> <p>You can exit the interactive session with:</p> <pre><code>exit\n</code></pre>"},{"location":"additional-guides/setting-a-time-limit/","title":"Setting a time limit","text":"<p><code>--time</code> is a useful tool to prevent hangs</p> <p>Sometimes, jobs may get stuck or encounter unforeseen issues, causing them to run indefinitely. Setting a time limit ensures that such jobs are automatically terminated after a certain duration, preventing them from consuming resources unnecessarily.</p> <p>You can add a <code>--time</code> parameter to your Slurm command, e.g. <code>--time=24:00:00</code> to run a job for a maximum of 24 hours, or <code>--time=1-8:00:00</code> to run a job for maximum 1 day and 8 hours. </p> <p>Example</p> <pre><code>srun --time=00:00:10 hostname\n</code></pre>"},{"location":"additional-guides/terminal-basics/","title":"Terminal basics","text":"<p>The following commands provide a solid foundation for navigating and working within a Linux terminal environment, particularly in the context of AI Student Cloud. Familiarize yourself with these commands to enhance your productivity and efficiency when using the platform. You can also look at this basic course to the shell at: https://linuxjourney.com/lesson/the-shell.</p>"},{"location":"additional-guides/terminal-basics/#navigation-commands","title":"Navigation Commands:","text":"<ul> <li><code>pwd</code>: Displays the current directory path.</li> <li><code>ls</code>: Shows the files and directories in the current location.</li> <li><code>cd [directory_name]</code>: Change to a directory within the current directory.</li> <li><code>cd /path/to/directory</code>: Change to an absolute directory path.</li> <li><code>cd ..</code>: Move up one directory level.</li> <li><code>cd ~</code>: Move to the home directory.</li> <li><code>cd -</code>: Return to the previous directory.</li> </ul>"},{"location":"additional-guides/terminal-basics/#file-management-commands","title":"File Management Commands:","text":"<ul> <li><code>mkdir [directory_name]</code>: Create a directory with a specific name.</li> <li><code>rmdir [directory_name]</code>: Remove a directory.</li> <li><code>touch [file_name]</code>: Create a file with a specific name.</li> <li><code>cp [source] [destination]</code>: Copy a file or directory from the source to the destination.</li> <li><code>mv [source] [destination]</code>: Move a file or directory to a new location or rename it.</li> <li><code>rm [file_name]</code>: Remove a file.</li> <li><code>rm -r [directory_name]</code>: Remove a directory and its contents recursively.</li> </ul>"},{"location":"additional-guides/terminal-basics/#text-editing-commands","title":"Text Editing Commands:","text":"<ul> <li><code>nano [file_name]</code>: Open a file for editing.<ul> <li><code>Ctrl + O</code>: Saving a File.</li> <li><code>Ctrl + X</code>: Exit Nano.</li> </ul> </li> <li><code>vim [file_name]</code>: Open a file for editing.<ul> <li><code>:q</code>: Exit Vim.</li> <li><code>:wq</code>: Exit and save Vim.</li> <li><code>i</code>: Enter insert mode, where you can type to insert or edit text. Press Esc to exit insert mode.</li> </ul> </li> </ul>"},{"location":"additional-guides/terminal-basics/#other-useful-commands","title":"Other Useful Commands:","text":"<ul> <li><code>cat [file_name]</code>: Display the contents of a file.</li> <li><code>grep [pattern] [file_name]</code>: Search for a specific pattern within a file.</li> <li><code>man [command_name]</code>: Access the manual pages for a specific command.</li> <li><code>history</code>: Display command history.</li> </ul>"},{"location":"additional-guides/for-lecturers/adding-a-course/","title":"Adding a course","text":"<p>You are invited to design your own course for the AI Student Cloud. You can create your own pages within the documentation and upload course materials directly to the AI Student Cloud server.</p>"},{"location":"additional-guides/for-lecturers/adding-a-course/#how-to-add-a-course-to-the-documentation","title":"How to add a course to the documentation","text":"<p>To include a course in the AI Student Cloud documentation, use Markdown format and Material for MkDocs for styling. Follow these steps to create a new course:</p> <ol> <li>Start by forking the repository to your GitHub account.</li> <li>In the <code>/docs/courses directory</code>, create a new folder named after your course. This folder will contain your <code>.md</code> files, which form the basis of your course content.<ul> <li>For design tips, refer to our page design guide.</li> <li>To preview your page, install Material for MkDocs by following this getting started guide.</li> <li>Your course series may include multiple files, e.g., <code>tensorflow-course-1.md</code>, <code>tensorflow-course-2.md</code>, etc.</li> </ul> </li> <li>Once you have made the necessary changes, submit a pull request to the main repository.</li> <li>We will review your changes and, if everything is in order, merge your pull request.</li> </ol>"},{"location":"additional-guides/for-lecturers/adding-a-course/#adding-image-files-to-ai-student-cloud","title":"Adding image files to AI Student Cloud","text":"<p>TODO</p>"},{"location":"additional-guides/for-lecturers/adding-a-course/#adding-course-material-to-ai-student-cloud","title":"Adding course material to AI Student Cloud","text":"<p>TODO</p>"},{"location":"additional-guides/for-lecturers/page-design-guide/","title":"Page design guide","text":"<p>This is a basic overview of which features you can use to design a MkDocs page. You can find more information at https://squidfunk.github.io/mkdocs-material/reference/.</p> Normal paragraph<pre><code>You write normal paragraph text just by typing like this. \n</code></pre> <p>You write normal paragraph text just by typing like this. </p> Headings<pre><code>You can create headings by placing a # before the text like so:. \n# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n</code></pre> Inserting links<pre><code>You can insert links by:\n\nI want to insert a link [here](https://www.researcher.aau.dk/contact/claaudia)\n</code></pre> <p>I want to insert a link here</p> Code in paragrapgh<pre><code>You use backticks to `write code in text`.\n</code></pre> <p>You use backticks to <code>write code in text</code>.</p> Console codeblocks<pre><code> You can also create a console codeblock like so:\n\n ```console\n ssh -l xxxxxx@student.aau.dk ai-fe02.srv.aau.dk\n ```\n</code></pre> <pre><code>ssh -l xxxxxx@student.aau.dk ai-fe02.srv.aau.dk\n</code></pre> Python codeblocks<pre><code> You can also create a Python codeblock like so:\n\n ```py\n import tensorflow as tf\n for i in range(len(100)):\n    print(i)\n ```\n</code></pre> <pre><code>import tensorflow as tf\n\nfor i in range(len(100)):\n    print(i)\n</code></pre> Call-outs<pre><code>You can make call-outs/admonitions like so:\n\n!!! info \"This is the title\"\n    This is the content\n\n!!! warning \"This is the title\"\n    This is the content\n</code></pre> <p>This is the title</p> <p>This is the content</p> <p>This is the title</p> <p>This is the content</p> <p>Look at https://squidfunk.github.io/mkdocs-material/reference/admonitions/ for more types of call-outs, like warning, example etc.</p> Data tables<pre><code>You can also create data tables like this:\n\n| Method      | Description     |\n| ----------- | --------------- |\n| `GET`       | Fetch resource  |\n| `PUT`       | Update resource |\n| `DELETE`    | Delete resource |\n</code></pre> Method Description <code>GET</code> Fetch resource <code>PUT</code> Update resource <code>DELETE</code> Delete resource Custom HTML/CSS<pre><code>&lt;div&gt;\n    &lt;p&gt;\n        You can paste your own custom HTML/CSS as you would normal in a .html file\n    &lt;/p&gt;\n&lt;/div&gt;\n</code></pre> <p>         You can paste your own custom HTML/CSS as you would normal in a .html file     </p> Inserting images<pre><code>You can insert images from urls or by uploading images to \"/assets/img/\":\n\n![Image of CLAAUDIA Logo](/assets/img/claaudia-logo.png)\n</code></pre> <p></p>"},{"location":"additional-guides/for-lecturers/page-design-guide/#heading-1","title":"Heading 1","text":""},{"location":"additional-guides/for-lecturers/page-design-guide/#heading-2","title":"Heading 2","text":""},{"location":"additional-guides/for-lecturers/page-design-guide/#heading-3","title":"Heading 3","text":""},{"location":"additional-guides/for-lecturers/page-design-guide/#heading-4","title":"Heading 4","text":""},{"location":"courses/customize-containers/","title":"Customize containers","text":"<p>To customize Singularity containers (.sif files) from sources like NGC, which are read-only, consider the following methods for incorporating additional software:</p> <ol> <li>Create a writable sandbox container image</li> <li>Build a new container image file with Cotainr</li> <li>Build a new container image file from a definition file</li> <li>Install Python packages with <code>pip</code> or <code>conda</code> outside the container</li> </ol>"},{"location":"courses/customize-containers/#1-create-a-writable-sandbox-container-image","title":"1. Create a writable sandbox container image","text":"<p>First create a so-called sandbox container: <pre><code>srun singularity build --sandbox [sandbox-dir-name] [container image]\n</code></pre> where you replace <code>[sandbox-dir-name]</code> with a name you decide for the directory that holds your sandbox container, and replace <code>[container image]</code> with the name of a container image you already have in AI Student Cloud (e.g. <code>pytorch_23.04-py3.sif</code>) or the URI of a container to download (e.g. <code>docker://nvcr.io/nvidia/pytorch:23.04-py3</code>).</p> Example <p>Start by creating a sandbox container:</p> <pre><code>srun singularity build --sandbox pytorch-23.04 docker://nvcr.io/nvidia/pytorch:23.04-py3\n</code></pre> <p>Then when you wish to install additional software in the container, open a shell in the container:</p> <pre><code>srun singularity shell --writable --fakeroot pytorch-23.04\n</code></pre> <p>While inside the container, install additional packages using for example the <code>apt</code> package manager: <pre><code>Singularity&gt; apt install [package name]\n</code></pre> or <code>pip</code> (for Python): <pre><code>Singularity&gt; pip install [package name]\n</code></pre></p>"},{"location":"courses/customize-containers/#2-build-a-new-container-image-file-with-cotainr","title":"2. Build a new container image file with Cotainr","text":"<p>Cotainr is a tool developed by DeiC to ease building of Singularity containers. It can be used to build custom containers with additional software installable by <code>conda</code> and <code>pip</code>. This means it is primarily for adding Python packages to a container.  It works from a base container image that you specify and then build additional Anaconda and Pip packages which you supply as a Conda environment specification.</p> <p>We begin by downloading the latest release from the Cotainr repository. In the example below we are downloading the latest version as of late 2023. Be sure to check for newer versions at the aforementioned repository. Look for the zip archive \"Assets\" section, and copy the link.</p> <pre><code>wget https://github.com/DeiC-HPC/cotainr/archive/refs/tags/2023.11.0.zip\n</code></pre> <p>You should now have a zip archive, which you can unzip with: <pre><code>unzip 2023.11.0.zip\n</code></pre></p> <p>After this has been done, you should have a directory called <code>cotainr-2023.11.0</code>. We should now be able to launch Cotainr and access its commands from within this directory. In a generalised manner the command structure is: <pre><code>srun [path/to/cotainr] build [name of output file] --base-image=[base image] --conda-env=[name of environment]\n</code></pre></p> <p>We use <code>srun</code> to ask Slurm to delegate the subsequent command to a compute node.  We then need to specify the path to <code>cotainr/bin/cotainr</code> and call <code>build</code>. Then choose a name for your container and replace <code>[name of output file]</code> with this newly chosen name. We recommend appending the conventional suffix <code>.sif</code> to this name. After that you will need to specify a <code>[base image]</code>, which can be an existing container in your directory or one from a remote source. Finally use the parameter <code>--conda-env</code> to specify which Conda environment file you want to use. If you have an existing Conda environment somewhere, you can export this environment <code>conda env export &gt; my_environemt.yml</code>.</p> Example <pre><code>srun ~/cotainr/bin/cotainr build amber.sif --base-image=docker://ubuntu:22.04 --conda-env=amber.yml\n</code></pre> <p>Also don't forget to check out the offical Cotainr documentation for more information.</p>"},{"location":"courses/customize-containers/#3-build-a-new-container-image-file-from-a-definition-file","title":"3. Build a new container image file from a definition file","text":"<p>You can also build containers from scratch or from an existing base image directly with Singularity. This is a somewhat more difficult approach than the above Cotainr tool, because it requires you to write the Singularity definition file yourself. The build process can be lengthy and so, it can take a long time with trial-and-error to get the definition right and produce a working container. Please see the Singularity documentation for details on how to build containers from definition files.</p>"},{"location":"courses/customize-containers/#4-install-python-packages-with-pip-or-conda-outside-the-container","title":"4. Install Python packages with pip or conda outside the container","text":"<p>This is not recommended, as the method is a bit \"brittle\". It is easy to set it up wrong and end up in a situation where versions of packages installed for different container images get mixed up and cause problems. Consider this a last resort: Please see this guide.</p>"},{"location":"courses/jupyter-notebook/","title":"Jupyter notebook","text":"<p>\u00b4srun --mem 50G singularity pull docker://nvcr.io/nvidia/pytorch:24.03-py3\u00b4</p> <p>\u00b4salloc\u00b4</p> <p>\u00b4ssh -L 9999:localhost:9999 a256-t4-02\u00b4</p> <p>\u00b4srun --pty singularity shell pytorch_24.03-py3.sif\u00b4</p> <p>salloc is a command used in Slurm, a workload manager used in high-performance computing (HPC) environments, to allocate resources for a job before it is executed.</p> <p>The salloc command is typically used to obtain an interactive shell session or to allocate resources for a specific task without submitting a job script. It allows users to request resources such as CPU cores, memory, GPUs, and time, and interactively run commands or launch applications within the allocated resources.</p> <p>Here's a basic usage of the salloc command:</p> <p>bash Copy code salloc --nodes=1 --ntasks-per-node=1 --time=1:00:00 In this example, salloc is used to allocate resources for a job on one node with one task per node, and a time limit of 1 hour.</p> <p>After running salloc, users typically obtain a shell prompt within the allocated resources, where they can interactively execute commands or launch applications. Once the allocated resources are no longer needed, users can exit the session, and the resources will be released back to the Slurm scheduler.</p>"},{"location":"courses/open-a-shell-in-a-container/","title":"Open a shell in a container","text":"<p>You can use <code>shell</code> command to open a shell in the container. This basically gives you a command line in the runtime environment inside the container where you can work interactively, i.e. type commands in the command line to run scripts and open applications. This is good for experimenting with how you wish to run your computations, analyses etc. in the container.</p> Example <pre><code>srun --pty singularity shell tensorflow_22.07-tf2-py3.sif\n</code></pre> <p>The <code>--pty</code> parameter is necessary in order to enable typing commands into the command line in the job. After opening the shell in the container, your command line terminal should display:</p> <pre><code>Singularity&gt;\n</code></pre> <p>This means that it is ready for you to type in commands. Type <code>exit</code> and hit ENTER to exit the container and stop the running job.</p>"},{"location":"getting-started/file-transfer/","title":"File transfer","text":"WindowsLinux/MacOS <p>For Windows users we recommend using the application WinSCP to transfer files from your local computer to AI Student Cloud. Other popular solutions are PuTTY and FileZilla. Alternatively, you can install OpenSSH to be able to use the <code>scp</code> command, shown for Linux/MacOS users.</p> <p>When you open WinSCP, you will be welcomed by a 'Login' modal. Follow the instructions in the image above to establish a connection to the server. You can now drag and drop files between your local computer and the AI Student Cloud platform.</p> <p></p> <p>Remember to change image with new server address</p> <p>Info</p> <p>You might want to display hidden files in WinSCP (such as files starting with a dot on Linux systems). Go to Options \u2192 Preferences... \u2192 Panels and turn on \"Show hidden files\".</p> <p>You can transfer files to/from AI Student Cloud using the command line utility <code>scp</code> from your local computer.</p> <p>Example</p> <pre><code>scp some-file xxxxxx@student.aau.dk@ai-fe02.srv.aau.dk:~\n</code></pre> <p>where '~' means your user directory on AI Student Cloud. </p> <p>You can append directories after that to your destination:</p> <pre><code>scp some-file xxxxxx@student.aau.dk@ai-fe02.srv.aau.dk:~/some-dir/some-sub-dir/\n</code></pre> <p>CHANGE ai-fe02.srv.aau.dk ADDRESS</p> <p>You can also copy in the opposite direction, e.g. from the AI Student Cloud platform to your local computer with:</p> <p>Example</p> <pre><code>scp xxxxxx@student.aau.dk@ai-fe02.srv.aau.dk:~/some-folder/some-subfolder/some-file .\n</code></pre> <p>where <code>.</code> means the current directory you are located in on your local computer. CHANGE ai-fe02.srv.aau.dk ADDRESS</p> <p>In general, file transfer tools that can use SSH as protocol should work. A common choice is FileZilla.</p> <p>Now that you know the basics of file transfer, lets proceed to learn how to get applications </p>"},{"location":"getting-started/getting-applications/","title":"Getting applications","text":"<p>To obtain applications on AI Student Cloud, you must utilize the container technology, Singularity. </p> <p>What is a container?</p> <p>A container is a stand-alone, executable software package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Read more about containers here.</p>"},{"location":"getting-started/getting-applications/#pre-downloaded-containers","title":"Pre-downloaded containers","text":"<p>The most straightforward method to acquire applications on AI Student Cloud is by accessing pre-downloaded containers stored within the platform. The default pre-downloaded containers presently include TensorFlow and PyTorch, located at PATH TO CLAAUDIA IMAGE DIRECTORY. We aim to consistently update these containers to the latest versions. </p> <p>Additionally, you can explore further pre-downloaded containers available in the courses directory PATH TO COURSES DIRECTORY on AI Student Cloud.</p> <p>Example</p> <p>First, let's check out which containers exist in PATH TO CLAAUDIA IMAGE DIRECTORY:</p> <pre><code>ls PATH TO CLAAUDIA IMAGE DIRECTORY\n</code></pre> <p>Lets copy the <code>tensorflow_24.03-tf2-py3.sif</code> container from PATH TO CLAAUDIA IMAGE DIRECTORY to your user directory. Note! TensorFlow might be newer version at this time.</p> <p>To copy a container from the directory, simply execute:</p> <pre><code>scp INSERT TENSORFLOW PATH xxxxxx@student.aau.dk@ai-fe02.srv.aau.dk:~\n</code></pre>"},{"location":"getting-started/getting-applications/#download-containers","title":"Download containers","text":"<p>Alternatively, you can access a wide array of containers by visiting NVIDIA GPU Cloud (NGC) and exploring whether NVIDIA provides a container for the application you require. Refer to our guide here for detailed instructions.</p>"},{"location":"getting-started/getting-applications/#customize-containers","title":"Customize containers","text":"<p>You also have the flexibility to create your own containers tailored to your specific environment requirements. Refer to our guide on creating a conda environment.</p> <p>Now that you know how to obtain applications, let's delve into running applications </p>"},{"location":"getting-started/login/","title":"Login","text":"<p>In practice, working in the AI Student Cloud primarily takes place via a command-line interface. You can access the platform using Secure Shell (SSH).</p> <p>The AI Student Cloud is only directly accessible when being on the AAU network (including VPN or AAU's SSH gateway). You can connect to AI Student Cloud front-end node by running the following command on the command line of your local Windows (Windows PowerShell), MacOS or Linux computer:</p> <p>Example</p> <pre><code>ssh -l xxxxxx@student.aau.dk ai-fe02.srv.aau.dk\n</code></pre> <p>Replace <code>xxxxxx@student.aau.dk</code> with your AAU email address.</p> <p>CHANGE ai-fe02.srv.aau.dk ADDRESS</p> <p>You are now ready to proceed to learn about file transfer </p>"},{"location":"getting-started/offboarding/","title":"Offboarding","text":"<p>The offboarding process of AI Student Cloud is important for promoting responsible usage, maintaining data privacy and security, and managing resources effectively to provide a seamless user experience.</p>"},{"location":"getting-started/offboarding/#clean-up-after-use","title":"Clean up after use","text":"<p>AI Student Cloud encourages responsible usage, which includes cleaning up your user directory for files that are no longer needed. Let's take a moment to tidy up the files associated with this \"Getting Started\" tutorial (...if you no longer need the files).</p> <pre><code>rm tensorflow_24.03-tf2-py3.sif\nrm benchmark_tensorflow.py\n</code></pre> <p>Before deleting files, remember that you can transfer files from your AI Student Cloud directory to your local computer.</p>"},{"location":"getting-started/offboarding/#deletion-of-data-at-the-end-of-the-semester","title":"Deletion of data at the end of the semester","text":"<p>At the semester's end, all your data and user information will be automatically removed from the AI Student Cloud platform. For the spring semester, the deletion date is June 31st, and for the autumn semester, December 31st. You will receive email notifications 1 month, 14 days, and 1 day prior to the deletion date. It is important to ensure that you transfer any desired files from the AI Student Cloud to your local computer before the deletion date.</p>"},{"location":"getting-started/offboarding/#apply-for-extension","title":"Apply for extension","text":"<p>Should you wish to keep your data on AI Student Cloud and use the platform for another semester, you can submit an extension request through the application formUPDATE for the AI Student Cloud. Alternatively, you can apply for a new project when the next semester begins.</p> <p> </p> <p> Congratulations! You have completed the basics of getting started with AI Student Cloud. As you continue your journey, we suggest you have a closer look at Additional Guides and CoursesUPDATE which contains more detailed examples of concrete use cases for the AI Student Cloud.</p> <p>For all questions and requests, please refer to the support page.</p>"},{"location":"getting-started/preperation/","title":"Preperation","text":"<p>Before diving into AI Student Cloud, it's essential to make sure you're equipped with the necessary tools and knowledge to make the most out of your experience. Here's a brief overview to get you started:</p> <p> Terminal Basics</p> <p>AI Student Cloud is accessed and managed primarily through the terminal. If you're new to working with the command line interface, we recommend to check Terminal Basics to get started.</p> <p> Windows users</p> <p>You'll need to use Windows PowerShell to follow our documentation effectively.</p> <p> Review Guidelines</p> <p>Before proceeding, we strongly recommend reviewing our Guidelines to ensure a smooth and productive experience for both yourself and other users of AI Student Cloud.</p> <p>With these preparations in place, Lets get started </p>"},{"location":"getting-started/running-applications/","title":"Running applications","text":"<p>Before you start running applications, it is important to be aware of AI Student Cloud queueing system Slurm.</p>"},{"location":"getting-started/running-applications/#slurm-queue-system","title":"Slurm queue system","text":"<p>Slurm is a job scheduling system and is used to allocate resources, manage user jobs, and provide a framework for job queueing, scheduling, and execution on AI Student Cloud. Applications on AI Student Cloud can only be run through Slurm. </p> <p>The simplest way to run a job via Slurm is to use the command <code>srun</code>.</p> <p>Example</p> <pre><code>srun hostname\n</code></pre> <p>This runs the command <code>hostname</code> as a job in the queue system. When run like this with no further parameters specified, Slurm will run the command on the first compute node available.  </p> <p>The command will return one of these host names. If the command displays <code>srun: job XXXXXX queued and waiting for resources</code>, this means that all compute nodes are fully occupied (by other users' jobs) and your job is waiting in queue to be executed when resources become available.</p> <p>This was your first Slurm job. You will need this (<code>srun</code>) and other Slurm commands for most of your work in AI Student Cloud.</p> <p>Checking the queue</p> <p>Before running a job you typically wish to see an overview of what is currently in the queue. For example to see how many jobs might be waiting ahead of you or to get an overview of your own jobs. We recommend looking at our guide Checking the queue to get familiar with Slurm queue commands.</p>"},{"location":"getting-started/running-applications/#running-a-container-job","title":"Running a container job","text":"<p>To run a container job, we can use the Singularity command <code>exec</code>. Lets try printing \"hello world\" using Python in the <code>tensorflow_24.03-tf2-py3.sif</code> container:</p> <pre><code>srun singularity exec tensorflow_24.03-tf2-py3.sif python3 -c \"print('hello world')\"\n</code></pre> <p>Info</p> <p>You can also run a container in interactive mode by using <code>shell</code> instead of <code>exec</code>. See this guide. NOT DONE</p>"},{"location":"getting-started/running-applications/#allocating-a-gpu-to-your-job","title":"Allocating a GPU to your job","text":"<p>The primary role of the AI Student Cloud is to run software that utilises one or more GPUs for computations. In order to run applications with a GPU you need to allocate a GPU to a job using Slurm. </p> <p>Let's try running a small Python script that performs a simple matrix multiplication of random data to benchmark TensorFlow computing speed:</p> <p>Example</p> <p>Download benchmark_tensorflow.py and copy it from your local computer to your user directory on AI Student Cloud using the <code>scp</code> command or <code>WinSCP</code> (see Step 2: File Transfer):</p> <p>Example</p> <p>Example</p> <p>As the last step, it's important to understand the process of Offboarding </p>"},{"location":"getting-started/running-applications/#execute-job-without-gpu","title":"Execute job without GPU","text":"<p>Now, lets first try executing <code>benchmark_tensorflow.py</code> without allocating a GPU:</p> <pre><code>srun singularity exec tensorflow_24.03-tf2-py3.sif python3 benchmark_tensorflow.py\n</code></pre> <p>After some time (ignore the warnings), the terminal should print something like: <code>Time taken: 21.1207 seconds</code>. This computation was performed on the CPU.</p>"},{"location":"getting-started/running-applications/#execute-with-gpu","title":"Execute with GPU","text":"<p>You can allocate a GPU to a job by using the <code>-G</code> or <code>--gres=gpu</code> option for Slurm. You also need to add <code>--nv</code> to enable NVIDIA GPUs in the container. Lets try allocating 1 arbitrary available GPU to the job by adding <code>--gres=gpu:1</code> and notice the time difference:</p> <pre><code>srun --gres=gpu:1 singularity exec --nv tensorflow_24.03-tf2-py3.sif python3 benchmark_tensorflow.py\n</code></pre> <p>Allocating more than one GPU?</p> <p>Note that the above example allocate 1 GPU to the job. It is possible to allocate more, for example <code>--gres=gpu:2</code> for two GPUs. Software for computing on GPU is not necessarily able to utilise more than one GPU at a time. It is your responsibility to ensure that the software you run can indeed utilise as many GPUs as you allocate. It is not allowed to allocate more GPUs than your job can utilise.</p>"},{"location":"getting-started/running-applications/#execute-with-specific-gpu","title":"Execute with specific GPU","text":"<p>In some cases your work requires a specific type of GPU. It could be, for example, that you need at least 20 GB of GPU RAM available. In that case at T4 GPU does not meet the requirement. It could also be that you know that an A10 GPU would be sufficient for your job, so there is no need to allocate an A40 GPU to it. Get an overview of the GPUs available in AI Student Cloud here.</p> <p>You can specify a specific type of GPU to allocate to your job. This is done by adding a GPU type label to the <code>--gres</code> option. Lets allocate 1 A10 GPU by adding <code>--gres=gpu:a10:1</code>:</p> <pre><code>srun --gres=gpu:a10:1 singularity exec --nv tensorflow_24.03-tf2-py3.sif python3 benchmark_tensorflow.py\n</code></pre> <p>Checking the status of compute nodes</p> <p>It is often desirable to monitor the resource status of the compute nodes when you wish to run a job on a certain GPU. We recommend looking at our guide Checking the status of compute nodes.</p> <p>UPDATE EXAMPLE WITH a10:1 TO MATCH AI STUDENT CLOUD GPU</p>"}]}